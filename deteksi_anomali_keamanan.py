# -*- coding: utf-8 -*-
"""Deteksi Anomali Keamanan - RevanaFS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13UemjiXnCkcwQJ3YUqyNp_PsQMUK-Jxg
"""

import pandas as pd                                               # Untuk manipulasi data (DataFrame, Series).
import numpy as np                                                # Untuk operasi numerik dan array.
import re                                                         # Untuk ekspresi reguler (pencarian pola teks).
import time                                                       # Untuk fungsi terkait waktu.
import sys                                                        # Untuk akses ke parameter dan fungsi sistem Python.
from urllib.parse import urlparse                                 # Untuk mengurai (mem-parsing) komponen URL.
from google.colab import drive                                    # Untuk menghubungkan Google Drive ke Colab.
from sklearn.ensemble import IsolationForest                      # Model deteksi anomali (Isolation Forest).
from sklearn.preprocessing import StandardScaler, OneHotEncoder   # Untuk penskalaan fitur (StandardScaler) dan encoding kategori (OneHotEncoder).
from sklearn.compose import ColumnTransformer                     # Untuk menerapkan transformer berbeda ke kolom berbeda pada data.

# Menghubungkan (mount) Google Drive ke direktori '/content/drive' di lingkungan Colab.
drive.mount('/content/drive')

# Path menuju file CSV yang akan diproses di Google Drive.
FILE_PATH = '/content/drive/MyDrive/dataset/part-00000-79fa9d08-cdc2-480e-9edf-1349b798379c-c000.csv' # Sesuaikan path ini dengan lokasi file Anda.

# Menentukan jumlah baris yang akan dibaca dari file CSV dalam satu waktu (per chunk/bagian).
CHUNKSIZE = 1000000

# Daftar nama kolom yang akan dipilih dan digunakan dari file CSV.
COLUMNS_TO_USE = ['url', 'domain', 'protocol']

# Daftar nama kolom yang jika bernilai null/kosong, maka seluruh baris tersebut akan dihapus.
COLUMNS_FOR_NULL_DROP = ['url', 'domain', 'protocol']

# Strategi: Ambil sampel dari tiap chunk, lalu pangkas jika total sampel melebihi batas.
# Proporsi data yang akan diambil sebagai sampel dari setiap chunk (misalnya, 0.05 berarti 5%).
SAMPLING_FRACTION_PER_CHUNK = 0.05

# Jumlah maksimum total baris yang diinginkan untuk sampel data gabungan (untuk deteksi anomali & pembelajaran terawasi).
# Jika total sampel dari semua chunk melebihi angka ini, sampel akan dipangkas secara acak.
MAX_TOTAL_SAMPLE_SIZE = 100000

# Angka seed untuk generator angka acak, memastikan hasil yang sama jika kode dijalankan ulang (reproduktifitas).
RANDOM_STATE = 42

# Estimasi proporsi anomali dalam dataset untuk model Isolation Forest.
# 'auto' berarti algoritma akan menentukan sendiri, atau bisa diisi dengan nilai float (misal, 0.01 untuk 1%).
IF_CONTAMINATION = 'auto'

# Fungsi untuk mendeteksi apakah sebuah domain merupakan domain pemerintah Indonesia atau domain .gov umum.
def is_gov_domain(domain_str):
    # Jika input domain adalah NaN (Not a Number) atau string 'nan', kembalikan False.
    if pd.isna(domain_str) or domain_str == 'nan':
        return False
    # Ubah input domain menjadi string dan huruf kecil untuk konsistensi.
    domain_str = str(domain_str).lower()
    # Periksa apakah domain diakhiri dengan .go.id (domain pemerintah Indonesia) atau .mil.id (domain militer Indonesia).
    if re.search(r'(\.go\.id$|\.mil\.id$)', domain_str):
        return True # Jika ya, kembalikan True.
    # Periksa apakah domain diakhiri dengan .gov (domain pemerintah umum) DAN BUKAN diakhiri dengan .go.id.
    if re.search(r'\.gov$', domain_str) and not re.search(r'\.go\.id$', domain_str):
        return True # Jika ya, kembalikan True.
    # Jika tidak ada kondisi di atas yang terpenuhi, kembalikan False.
    return False

# Fungsi untuk mengekstrak berbagai fitur turunan (engineered features) dari sebuah chunk DataFrame.
def combined_engineer_features(df_chunk):
    # Membuat DataFrame kosong 'engineered_df' dengan indeks yang sama seperti df_chunk untuk menyimpan fitur baru.
    engineered_df = pd.DataFrame(index=df_chunk.index)

    # Memastikan kolom dasar ('url', 'domain', 'protocol') ada di df_chunk.
    # Jika tidak ada, kolom tersebut dibuat kosong dan diisi string kosong untuk menghindari error.
    # Jika ada, diubah tipenya menjadi string dan NaN diisi string kosong.
    for col in ['url', 'domain', 'protocol']:     # Loop untuk setiap kolom dasar.
        if col not in df_chunk.columns:           # Jika kolom tidak ada di df_chunk.
            # Cetak peringatan bahwa kolom tidak ditemukan.
            print(f"Peringatan: Kolom '{col}' tidak ada di chunk. Fitur terkait akan bernilai default/NaN.")
            df_chunk[col] = ""                    # Buat kolom kosong di df_chunk.
        else:                                     # Jika kolom ada.
            # Ubah tipe data kolom menjadi string dan isi nilai NaN dengan string kosong.
            df_chunk[col] = df_chunk[col].astype(str).fillna('')

    # Hitung panjang string domain.
    engineered_df['domain_length'] = df_chunk['domain'].str.len()
    # Hitung jumlah subdomain (jumlah titik dalam domain).
    engineered_df['num_subdomains'] = df_chunk['domain'].str.count('\\.')
    # Deteksi apakah domain adalah domain pemerintah (menggunakan fungsi is_gov_domain) dan ubah hasilnya (True/False) menjadi integer (1/0).
    engineered_df['is_gov_domain_ml'] = df_chunk['domain'].apply(is_gov_domain).astype(int)
    # Buat pola regex untuk mendeteksi format alamat IP.
    ip_pattern_domain = re.compile(r"^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$")
    # Periksa apakah domain merupakan alamat IP, hasilkan 1 jika ya, 0 jika tidak.
    engineered_df['domain_is_ip'] = df_chunk['domain'].apply(lambda x: 1 if ip_pattern_domain.match(x) else 0)

    # Hitung panjang string URL.
    engineered_df['url_length'] = df_chunk['url'].str.len()
    # Hitung jumlah karakter spesial dalam URL (selain huruf, angka, spasi, dan beberapa karakter URL umum).
    engineered_df['url_num_special_chars'] = df_chunk['url'].str.count(r'[^a-zA-Z0-9\s:/.\-?=&#]')
    # Periksa apakah URL mengandung format alamat IP, hasilkan 1 jika ya, 0 jika tidak.
    engineered_df['url_contains_ip'] = df_chunk['url'].str.contains(r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}', regex=True).astype(int)

    # Fungsi pembantu untuk menghitung jumlah segmen path dalam URL secara aman (menghindari error).
    def count_path_segments_safe(url_str):
        try:
            # Hitung jumlah segmen path (dipisahkan oleh '/') yang tidak kosong.
            return len([seg for seg in urlparse(url_str).path.split('/') if seg])
        except Exception: # Jika terjadi error.
            return 0
    # Terapkan fungsi count_path_segments_safe ke setiap URL untuk mendapatkan jumlah segmen path.
    engineered_df['num_path_segments'] = df_chunk['url'].apply(count_path_segments_safe)

    # Fungsi untuk menghitung jumlah parameter query dalam URL secara aman.
    def count_query_params_safe(url_str):
        try:
            # Jika ada query string, hitung jumlah parameter (dipisahkan oleh '&'). Jika tidak, kembalikan 0.
            return len(urlparse(url_str).query.split('&')) if urlparse(url_str).query else 0
        except Exception: # Jika terjadi error.
            return 0
    # Terapkan fungsi count_query_params_safe ke setiap URL untuk mendapatkan jumlah parameter query.
    engineered_df['num_query_params'] = df_chunk['url'].apply(count_query_params_safe)

    # Daftar kata kunci yang dianggap mencurigakan dalam URL.
    suspicious_keywords = ['eval\(', 'base64_decode\(', 'exec\(', '\.php\?', '\.exe', 'cmd\.exe', '/bin/bash', 'DROP TABLE', 'SELECT \* FROM']
    # Gabungkan daftar kata kunci menjadi satu pola regex (dipisahkan oleh '|' yang berarti ATAU).
    keyword_pattern = '|'.join(suspicious_keywords)
    # Periksa apakah URL mengandung salah satu kata kunci mencurigakan (case-insensitive), hasilkan 1 jika ya, 0 jika tidak.
    engineered_df['url_contains_suspicious_keyword'] = df_chunk['url'].str.contains(keyword_pattern, case=False, regex=True).astype(int)

    # Salin kolom 'protocol' asli ke kolom baru 'protocol_feature' untuk digunakan dalam OneHotEncoding nanti.
    # Ini dilakukan untuk menghindari konflik nama jika kolom 'protocol' asli juga ingin dipertahankan.
    engineered_df['protocol_feature'] = df_chunk['protocol']

    # Buat salinan kolom-kolom asli yang digunakan (COLUMNS_TO_USE) untuk inspeksi atau analisis lebih lanjut.
    original_data_for_inspection = df_chunk[COLUMNS_TO_USE].copy()

    # Gabungkan DataFrame berisi data asli ('original_data_for_inspection') dengan DataFrame berisi fitur baru ('engineered_df').
    # axis=1 berarti penggabungan dilakukan berdasarkan kolom.
    return pd.concat([original_data_for_inspection, engineered_df], axis=1)

# Fungsi untuk mendefinisikan variabel target 'is_suspicious' berdasarkan berbagai aturan heuristik.
def define_target_variable(df_chunk, malicious_domains_set=None):
    # Buat salinan dari df_chunk untuk dimodifikasi. df_chunk diasumsikan sudah memiliki fitur dari combined_engineer_features.
    df_target = df_chunk.copy()
    # Inisialisasi kolom 'is_suspicious' dengan nilai 0 (tidak mencurigakan) untuk semua baris.
    df_target['is_suspicious'] = 0

    # Terapkan aturan heuristik untuk menandai baris sebagai 'is_suspicious' (bernilai 1).
    # Jika kolom 'url_contains_suspicious_keyword' ada dan nilainya 1, tandai sebagai mencurigakan.
    if 'url_contains_suspicious_keyword' in df_target.columns:
        df_target.loc[df_target['url_contains_suspicious_keyword'] == 1, 'is_suspicious'] = 1
    # Jika kolom 'domain_is_ip' ada dan nilainya 1 (domain adalah IP), tandai sebagai mencurigakan.
    if 'domain_is_ip' in df_target.columns:
        df_target.loc[df_target['domain_is_ip'] == 1, 'is_suspicious'] = 1

    # Aturan tambahan: Jika protokol bukan 'https', 'ssl', atau 'http', tandai sebagai mencurigakan.
    if 'protocol' in df_target.columns:
        # Tandai baris sebagai mencurigakan jika nilai 'protocol' (setelah diubah ke huruf kecil) tidak ada dalam daftar ['https', 'ssl', 'http'].
        df_target.loc[~df_target['protocol'].str.lower().isin(['https', 'ssl', 'http']), 'is_suspicious'] = 1

    # Jika ada set domain berbahaya (malicious_domains_set) dan kolom 'domain' ada.
    if malicious_domains_set and 'domain' in df_target.columns: # Periksa keberadaan kolom 'domain' asli.
        # Tandai baris sebagai mencurigakan jika domainnya ada dalam 'malicious_domains_set'.
        df_target.loc[df_target['domain'].isin(malicious_domains_set), 'is_suspicious'] = 1
    # Kembalikan DataFrame yang sudah memiliki kolom target 'is_suspicious'.
    return df_target

# Inisialisasi sebuah set kosong untuk menyimpan domain berbahaya.
malicious_domains_set = set()

start_time_processing = time.time()   # Catat waktu mulai proses untuk mengukur durasi.
all_processed_samples_list = []       # List kosong untuk menyimpan semua sampel DataFrame yang telah diproses dari setiap chunk.
total_rows_read = 0                   # Variabel untuk menghitung total baris yang dibaca dari file.
total_rows_after_dropna = 0           # Variabel untuk menghitung total baris setelah penghapusan baris dengan nilai null.
total_rows_sampled_from_chunks = 0    # Variabel untuk menghitung total baris sampel yang terkumpul dari semua chunk.
processed_chunks_count = 0            # Variabel untuk menghitung jumlah chunk yang telah diproses.

try:# Blok try-except untuk menangani potensi error saat pemrosesan file.
    # Baca file CSV secara bertahap (per chunk) menggunakan iterator.
    # Hanya kolom yang didefinisikan di COLUMNS_TO_USE yang dibaca.
    # low_memory=False untuk mencegah masalah tipe data, on_bad_lines='skip' untuk melewati baris yang error.
    file_iterator = pd.read_csv(FILE_PATH, chunksize=CHUNKSIZE, usecols=COLUMNS_TO_USE, low_memory=False, on_bad_lines='skip')
    # Loop untuk setiap chunk data yang dibaca dari file.
    for chunk in file_iterator:
        processed_chunks_count += 1 # Tambah hitungan chunk yang diproses.
        print(f"   Memproses chunk ke-{processed_chunks_count}, {len(chunk)} baris...")
        total_rows_read += len(chunk) # Akumulasi total baris yang dibaca.

        # Hapus baris dalam chunk yang memiliki nilai null pada kolom yang ditentukan di COLUMNS_FOR_NULL_DROP.
        chunk.dropna(subset=COLUMNS_FOR_NULL_DROP, inplace=True)
        if chunk.empty: # Jika chunk menjadi kosong setelah penghapusan baris null.
            print(f"     Chunk ke-{processed_chunks_count} kosong setelah penghapusan baris null.") # Info.
            continue # Lanjutkan ke chunk berikutnya.
        total_rows_after_dropna += len(chunk) # Akumulasi total baris setelah dropna.

        for col in COLUMNS_TO_USE: # Loop untuk setiap kolom yang digunakan.
            if col in chunk.columns: # Jika kolom ada dalam chunk.
                # Ubah tipe data menjadi string, ubah ke huruf kecil, dan hapus spasi di awal/akhir.
                chunk[col] = chunk[col].astype(str).str.lower().str.strip()

        # Panggil fungsi combined_engineer_features untuk membuat fitur-fitur baru dari chunk.
        featured_chunk = combined_engineer_features(chunk)

        # Selalu panggil define_target_variable untuk membuat kolom target 'is_suspicious'.
        # malicious_domains_set akan kosong jika tidak ada file sumber domain berbahaya yang dikonfigurasi.
        final_chunk_for_sampling = define_target_variable(featured_chunk, malicious_domains_set)

        # Jika fraksi sampling valid (antara 0 dan 1, tidak termasuk 1).
        if SAMPLING_FRACTION_PER_CHUNK < 1.0 and SAMPLING_FRACTION_PER_CHUNK > 0:
            # Ambil sampel acak dari chunk berdasarkan fraksi yang ditentukan.
            sampled_chunk = final_chunk_for_sampling.sample(frac=SAMPLING_FRACTION_PER_CHUNK, random_state=RANDOM_STATE)
        else: # Jika fraksi sampling adalah 1.0 (ambil semua) atau tidak valid (ambil semua sebagai default).
            sampled_chunk = final_chunk_for_sampling # Gunakan seluruh chunk sebagai sampel.

        # Tambahkan DataFrame sampel dari chunk ini ke list utama.
        all_processed_samples_list.append(sampled_chunk)
        total_rows_sampled_from_chunks += len(sampled_chunk) # Akumulasi total baris sampel.
        # Info progres sampling per chunk.
        print(f"     Chunk {processed_chunks_count} selesai. Sampel terkumpul dari chunk: {len(sampled_chunk)}. Total sampel sejauh ini: {total_rows_sampled_from_chunks}")

    del file_iterator # Hapus iterator file untuk membebaskan memori setelah selesai.
    print(f"\nSelesai membaca dan memproses {processed_chunks_count} chunk.")                 # Ringkasan jumlah chunk.
    print(f"Total baris dibaca dari file: {total_rows_read}")                                 # Ringkasan total baris dibaca.
    print(f"Total baris setelah penghapusan null: {total_rows_after_dropna}")                 # Ringkasan total baris setelah dropna.
    print(f"Total baris sampel terkumpul dari semua chunk: {total_rows_sampled_from_chunks}") # Ringkasan total sampel.

except FileNotFoundError: # Jika file tidak ditemukan pada path yang diberikan.
    print(f"ERROR: File tidak ditemukan di path: {FILE_PATH}") # Pesan error.
    sys.exit() # Hentikan program.
except Exception as e: # Jika terjadi error lain saat membaca atau memproses file.
    print(f"ERROR: Terjadi kesalahan saat membaca atau memproses file: {e}") # Pesan error spesifik.
    sys.exit() # Hentikan program.

# Periksa apakah ada data sampel yang berhasil dikumpulkan.
if not all_processed_samples_list: # Jika list sampel kosong.
    print("\nTidak ada data sampel yang terkumpul. Program akan berhenti.") # Pesan error.
    sys.exit() # Hentikan program.

# Gabungkan semua DataFrame sampel dari 'all_processed_samples_list' menjadi satu DataFrame master.
master_sample_df = pd.concat(all_processed_samples_list, ignore_index=True)

# Cetak informasi ukuran sampel gabungan sebelum dilakukan pemangkasan akhir (jika perlu).
print(f"\nUkuran sampel gabungan sebelum pemangkasan akhir: {len(master_sample_df)} baris.")

# Jika ukuran sampel gabungan ('master_sample_df') melebihi batas maksimum yang ditentukan ('MAX_TOTAL_SAMPLE_SIZE').
if len(master_sample_df) > MAX_TOTAL_SAMPLE_SIZE:
    # Cetak informasi bahwa sampel akan dipangkas.
    print(f"Memangkas sampel menjadi {MAX_TOTAL_SAMPLE_SIZE} baris.")
    # Lakukan sampling acak untuk mengambil sejumlah 'MAX_TOTAL_SAMPLE_SIZE' baris.
    master_sample_df = master_sample_df.sample(n=MAX_TOTAL_SAMPLE_SIZE, random_state=RANDOM_STATE).reset_index(drop=True)

# Jika DataFrame sampel master kosong setelah proses penggabungan atau pemangkasan.
if master_sample_df.empty:
    # Cetak pesan error bahwa tidak ada data sampel yang tersedia.
    print("Sampel data untuk ML kosong setelah penggabungan/pemangkasan.")
    sys.exit() # Hentikan program.

# Catat waktu selesai proses untuk menghitung total durasi pemrosesan dan sampling.
end_time_processing = time.time()

# Cetak total durasi pemrosesan data dan sampling (difformat 2 angka di belakang koma).
print(f"Pemrosesan data dan sampling selesai dalam {end_time_processing - start_time_processing:.2f} detik.")

# Cetak informasi ukuran akhir dataset sampel yang akan digunakan untuk Machine Learning (jumlah baris dan kolom).
print(f"Ukuran dataset sampel akhir untuk ML: {master_sample_df.shape[0]} baris, {master_sample_df.shape[1]} kolom.")

# Cetak header untuk menampilkan contoh data.
print("\nContoh 5 baris pertama dari data sampel akhir:")

# Tampilkan 5 baris pertama dari DataFrame sampel akhir. .to_string() untuk memastikan semua kolom ditampilkan.
print(master_sample_df.head().to_string())

# DETEKSI ANOMALI (ISOLATION FOREST)

# Pilih kolom numerik dari master_sample_df untuk digunakan sebagai fitur dalam Isolation Forest.
numerical_features_if = master_sample_df.select_dtypes(include=np.number).columns.tolist()

# Hapus kolom target atau kolom hasil prediksi anomali sebelumnya jika ada dalam daftar fitur numerik.
if 'is_suspicious' in numerical_features_if: numerical_features_if.remove('is_suspicious')
if 'anomaly_score' in numerical_features_if: numerical_features_if.remove('anomaly_score')
if 'is_anomaly' in numerical_features_if: numerical_features_if.remove('is_anomaly')
if 'is_anomaly_pred_if' in numerical_features_if: numerical_features_if.remove('is_anomaly_pred_if')

# Tentukan fitur kategorikal yang akan digunakan oleh Isolation Forest.
categorical_features_if = ['protocol_feature']

# Cetak daftar fitur numerik dan kategorikal yang akan digunakan.
print(f"Fitur numerik untuk Isolation Forest: {numerical_features_if}")
print(f"Fitur kategorikal untuk Isolation Forest: {categorical_features_if}")

# Buat salinan DataFrame hanya dengan fitur-fitur yang akan digunakan untuk Isolation Forest.
X_for_if = master_sample_df[numerical_features_if + categorical_features_if].copy()

# Penanganan Nilai NaN pada Fitur
# Loop untuk setiap fitur numerik.
for col in numerical_features_if:
    if X_for_if[col].isnull().any():        # Jika ada nilai NaN dalam kolom numerik.
        median_val = X_for_if[col].median() # Hitung median dari kolom tersebut.
        # Cetak peringatan dan informasi pengisian NaN dengan median.
        print(f"Peringatan IF: Kolom numerik '{col}' memiliki NaN, diisi dengan median ({median_val}).")
        X_for_if[col].fillna(median_val, inplace=True) # Isi NaN dengan median.
# Loop untuk setiap fitur kategorikal.
for col in categorical_features_if:
    if X_for_if[col].isnull().any(): # Jika ada nilai NaN dalam kolom kategorikal.
        # Cetak peringatan dan informasi pengisian NaN dengan string 'missing'.
        print(f"Peringatan IF: Kolom kategorikal '{col}' memiliki NaN, diisi dengan 'missing'.")
        X_for_if[col].fillna('missing', inplace=True) # Isi NaN dengan 'missing'.

# Pra-pemrosesan Fitur (StandardScaler untuk numerik, OneHotEncoder untuk kategorikal)
# Definisikan preprocessor menggunakan ColumnTransformer.
preprocessor_if = ColumnTransformer(
    transformers=[
        # Terapkan StandardScaler ke fitur numerik.
        ('num', StandardScaler(), numerical_features_if),
        # Terapkan OneHotEncoder ke fitur kategorikal. handle_unknown='ignore' untuk mengabaikan nilai baru saat transformasi.
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features_if)
    ],
    remainder='drop' # Abaikan kolom lain yang tidak didefinisikan dalam transformers.
)

# Jika DataFrame fitur X_for_if kosong.
if X_for_if.empty:
    print("Data fitur untuk Isolation Forest kosong. Tidak bisa melanjutkan bagian ini.")
else: # Jika DataFrame fitur tidak kosong.
    # Terapkan pra-pemrosesan (fit dan transform) pada data fitur.
    X_if_processed = preprocessor_if.fit_transform(X_for_if)

    # Cetak dimensi data fitur setelah pra-pemrosesan.
    print(f"Dimensi data fitur IF setelah pra-pemrosesan: {X_if_processed.shape}")

    # Jika data fitur yang diproses memiliki baris (tidak kosong).
    if X_if_processed.shape[0] > 0:
        # Inisialisasi model Isolation Forest.
        model_iso = IsolationForest(n_estimators=100, contamination=IF_CONTAMINATION, random_state=RANDOM_STATE, n_jobs=-1)
        # Latih model Isolation Forest dengan data fitur yang sudah diproses.
        model_iso.fit(X_if_processed)

        # Hitung skor anomali untuk setiap data point. Skor lebih rendah berarti lebih anomali.
        master_sample_df['anomaly_score'] = model_iso.decision_function(X_if_processed)
        # Prediksi apakah setiap data point adalah anomali (-1) atau inlier (1).
        master_sample_df['is_anomaly_pred_if'] = model_iso.predict(X_if_processed)
        # Ubah hasil prediksi menjadi format biner (1 untuk anomali, 0 untuk inlier).
        master_sample_df['is_anomaly'] = master_sample_df['is_anomaly_pred_if'].apply(lambda x: 1 if x == -1 else 0)

        print("\nHASIL DETEKSI ANOMALI (Isolation Forest):") # Header hasil.
        # Filter DataFrame untuk mendapatkan hanya baris yang terdeteksi sebagai anomali, urutkan berdasarkan skor anomali.
        anomalies_df = master_sample_df[master_sample_df['is_anomaly'] == 1].sort_values(by='anomaly_score', ascending=True)
        # Cetak total anomali yang terdeteksi dan proporsinya.
        print(f"Total anomali terdeteksi berjumlah {len(anomalies_df)} dari {len(master_sample_df)} baris (proporsi: {len(anomalies_df)/len(master_sample_df):.2%}).")

        # Jika ada anomali yang terdeteksi.
        if not anomalies_df.empty:
            # Cetak header untuk contoh anomali.
            print(f"\nContoh {min(15, len(anomalies_df))} anomali teratas (berdasarkan skor terendah):")
            # Daftar kolom yang ingin ditampilkan untuk anomali.
            cols_to_display_if = ['url', 'domain', 'protocol', 'anomaly_score', 'is_gov_domain_ml',
                                  'domain_length', 'num_subdomains', 'domain_is_ip',
                                  'url_length', 'num_path_segments', 'num_query_params', 'protocol_feature']
            # Filter daftar kolom agar hanya berisi kolom yang memang ada di DataFrame anomalies_df.
            display_cols_existing_if = [col for col in cols_to_display_if if col in anomalies_df.columns]
            # Tampilkan N anomali teratas (maksimal 15) dengan kolom yang dipilih.
            print(anomalies_df[display_cols_existing_if].head(min(15, len(anomalies_df))).to_string())
        else: # Jika tidak ada anomali yang terdeteksi.
            print("Tidak ada anomali yang terdeteksi dengan konfigurasi model Isolation Forest saat ini.")
    else: # Jika tidak ada data yang tersisa setelah pra-pemrosesan.
        print("Tidak ada data yang tersisa untuk Isolation Forest setelah pra-pemrosesan.")